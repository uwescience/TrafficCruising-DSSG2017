{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training: Cruising Identification\n",
    "### Written by Orysya Stus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import json\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MULTI-STEP CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = !ls *.json\n",
    "one_week = []\n",
    "for d in data:\n",
    "    with open(d) as f:\n",
    "        resulting_json = json.load(f)\n",
    "        one_week.append(pd.DataFrame(resulting_json))\n",
    "metadata = pd.concat(one_week, axis=0)\n",
    "metadata = metadata.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if duplicate hashes exist and remove unneeded columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Duplicate checker\n",
    "print('Number of instances: ', metadata.shape[0])\n",
    "print('Number of unique hashes: ', metadata['group'].nunique())\n",
    "a = pd.DataFrame(metadata.groupby('group')['group'].count())\n",
    "group_duplicates = list(a.index[a['group'] > 1])\n",
    "print('Number of duplicates: ', len(group_duplicates))\n",
    "index_hash_mapping = dict(zip(list(metadata.index), list(metadata['group'])))\n",
    "del metadata['group']\n",
    "del metadata['id']\n",
    "del metadata['reduction']\n",
    "del metadata['timeofday']\n",
    "del metadata['weekday']\n",
    "del metadata['date']\n",
    "del metadata['fhv']\n",
    "\n",
    "# # These features would not provide additional information gain\n",
    "del metadata['distance_shortest']\n",
    "del metadata['distance_total']\n",
    "del metadata['duration_of_trip']\n",
    "metadata = metadata.astype(float)\n",
    "metadata['speed_average'] = metadata['speed_average']*2.23694\n",
    "metadata['speed_max'] = metadata['speed_max']*2.23694\n",
    "metadata['speed_standard_deviation'] = metadata['speed_standard_deviation']*2.23694"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('For modeling, the shape of the metadata is: ', metadata.shape)\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of Metadata\n",
    "    Summary Statistics\n",
    "    Attribute Histograms\n",
    "    Attribute Correlations\n",
    "    Attribute Covariances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.hist(bins=100, figsize=(20,20));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.cov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from pandas.tools.plotting import scatter_matrix\n",
    "# scatter_matrix(metadata, alpha=0.03, figsize=(20, 20), diagonal='kde');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data Creation\n",
    "Not Cruising:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('time_percentage_driving == 0 and time_percentage_bogus == 0 count: ', metadata[(metadata['time_percentage_driving'] == 0) & (metadata['time_percentage_bogus'] == 0)].shape[0])\n",
    "print('time_percentage_driving == 0 and time_percentage_bogus == 0 %: ', round(100.0*metadata[(metadata['time_percentage_driving'] == 0) & (metadata['time_percentage_bogus'] == 0)].shape[0]/metadata.shape[0], 2),'%')\n",
    "\n",
    "print('\\ntime_percentage_walking == 100 count: ', metadata['time_percentage_walking'][metadata['time_percentage_walking'] == 100].count())\n",
    "print('time_percentage_walking == 100 %: ', round(100.0*metadata['time_percentage_walking'][metadata['time_percentage_walking'] == 100].count()/metadata.shape[0], 2))\n",
    "\n",
    "print('\\nMark above cases as not cruising, to use as training data.')\n",
    "not_cruising = []\n",
    "not_cruising.extend(list(metadata[(metadata['time_percentage_driving'] == 0) & (metadata['time_percentage_bogus'] == 0)].index))\n",
    "not_cruising.extend(list(metadata['time_percentage_walking'][metadata['time_percentage_walking'] == 100].index))\n",
    "\n",
    "not_cruising = list(set(not_cruising))\n",
    "print('Not cruising count: ', len(not_cruising))\n",
    "print('Not cruising %: ', round(100.0*len(not_cruising)/metadata.shape[0], 2),'%')\n",
    "metadata1 = metadata[~metadata.index.isin(not_cruising)]\n",
    "\n",
    "#Get data where threshold == 1  --> not cruising\n",
    "not_cruising_threshold = 1\n",
    "print('\\nNumber of instances where distance_ratio ==', not_cruising_threshold,': ', metadata1['distance_ratio'][metadata1['distance_ratio'] == not_cruising_threshold].count())\n",
    "print('% of instances where distance_ratio ==', not_cruising_threshold, ': ', round(100.0*metadata1['distance_ratio'][metadata1['distance_ratio'] == not_cruising_threshold].count()/metadata.shape[0], 2), '%')\n",
    "not_cruising.extend(list(metadata1[metadata1['distance_ratio'] == not_cruising_threshold].index))\n",
    "\n",
    "print('\\n*********************For Training: Not Cruising********************* \\nTotal not cruising count: ', len(not_cruising))\n",
    "print('Total not cruising %: ', round(100.0*len(not_cruising)/metadata.shape[0], 2),'%')\n",
    "\n",
    "metadata1 = metadata[~metadata.index.isin(not_cruising)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cruising:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get data where threshold < 0.3 --> cruising\n",
    "cruising_threshold = 0.3\n",
    "print('\\n*********************For Training: Cruising********************* \\ndistance_ratio <', cruising_threshold,'count: ', metadata1['distance_ratio'][metadata1['distance_ratio'] < cruising_threshold].count())\n",
    "print('distance_ratio <', cruising_threshold, '%: ', round(100.0*metadata1['distance_ratio'][metadata1['distance_ratio'] < cruising_threshold].count()/metadata.shape[0], 2), '%')\n",
    "cruising = list(metadata1[metadata1['distance_ratio'] < cruising_threshold].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import statsmodels.api as sm\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import cross_val_score, cross_val_predict, StratifiedKFold \n",
    "from sklearn import preprocessing, metrics, svm, ensemble\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating data between known (labeled for training) and unknowns (need to be predicted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_cruising_data = metadata[metadata.index.isin(not_cruising)]\n",
    "not_cruising_data['label'] = 'not_cruising'\n",
    "cruising_data = metadata[metadata.index.isin(cruising)]\n",
    "cruising_data['label'] = 'cruising'\n",
    "\n",
    "knowns = pd.concat([not_cruising_data, cruising_data])\n",
    "unknowns = metadata[(~metadata.index.isin(not_cruising)) & (~metadata.index.isin(cruising))]\n",
    "del knowns['distance_ratio']\n",
    "del unknowns['distance_ratio']\n",
    "print('Knowns: ', knowns.shape, 'Unknowns: ', unknowns.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build training dataset for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_data(dataframe, standardize, random_state):\n",
    "    \"\"\"The function prepares the train & test dataframes for modeling.\n",
    "    Standardizing the data is desired.\"\"\"\n",
    "    data = np.array(dataframe.ix[:, dataframe.columns != 'label'])\n",
    "    dic = {'cruising':1, 'not_cruising':0}\n",
    "    target = np.array([dic[n] if n in dic else n for n in np.array(dataframe.ix[:, -1])])\n",
    "        \n",
    "    if standardize == 'True':\n",
    "        \"\"\"Scale the data (Assume that all features are centered around 0 and have variance in the same order. If a \n",
    "        feature has a variance that is orders of magnitude larger that others, it might dominate the objective \n",
    "        function and make the estimator unable to learn from other features correctly as expected). Note in order for \n",
    "        StandardScaler to work, need to remove any nulls in data set prior to running.\"\"\"\n",
    "        scalar = preprocessing.StandardScaler().fit(data)\n",
    "        data = scalar.transform(data)\n",
    "        joblib.dump(scalar, 'standardize_X.pkl')\n",
    "    else: pass\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.4, random_state=random_state)\n",
    "    return(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "accuracy = []\n",
    "roc = []\n",
    "f1_score = []\n",
    "recall = []\n",
    "precision = []\n",
    "for i in range(10):\n",
    "    print('\\n********************** For round', i, 'validation set. ********************** ')\n",
    "    X_train, y_train, X_test, y_test = training_data(knowns, 'False', i)\n",
    "    kf = StratifiedKFold(y_train, n_folds=10, shuffle=True, random_state=0)\n",
    "    gridparams = dict(criterion=['gini', 'entropy'], max_depth=[2,3,4,5,6,7,8,9,10])\n",
    "    params = {'random_state': None}\n",
    "    dt = GridSearchCV(DecisionTreeClassifier(**params), gridparams, cv=kf, scoring='f1', n_jobs=-1)\n",
    "    \n",
    "    # Training the model\n",
    "    dt.fit(X_train, y_train)\n",
    "    print('Best model: ')\n",
    "    print(dt.best_estimator_)\n",
    "    print(\"\")\n",
    "    \n",
    "    # Predicting on the test set\n",
    "    y_pred = dt.predict(X_test)\n",
    "    \n",
    "    # Scoring the performance of the model\n",
    "    print(\"Accuracy: %0.3f\" % accuracy_score(y_test, y_pred, normalize=True))\n",
    "    print(\"Aucroc: %0.3f\" % metrics.roc_auc_score(y_test, y_pred))\n",
    "    print(\"f1 score: %0.3f\" % metrics.f1_score(y_test, y_pred))\n",
    "    print(\"Recall: %0.3f\" % metrics.recall_score(y_test, y_pred))\n",
    "    print(\"Precision: %0.3f\" % metrics.precision_score(y_test, y_pred))\n",
    "    accuracy.append(accuracy_score(y_test, y_pred, normalize=True))\n",
    "    roc.append(metrics.roc_auc_score(y_test, y_pred))\n",
    "    f1_score.append(metrics.f1_score(y_test, y_pred))\n",
    "    recall.append(metrics.recall_score(y_test, y_pred))\n",
    "    precision.append(metrics.precision_score(y_test, y_pred))    \n",
    "\n",
    "print('\\n********************** Decision tree performance ********************** ')\n",
    "print('The accuracy for this model is: %0.3f +/- %0.3f' % (mean(accuracy), std(accuracy)))\n",
    "print('The auc_roc for this model is: %0.3f +/- %0.3f' % (mean(roc), std(accuracy)))\n",
    "print('The precision for this model is: %0.3f +/- %0.3f' % (mean(precision), std(accuracy)))\n",
    "print('The recall for this model is: %0.3f +/- %0.3f' % (mean(recall), std(accuracy)))\n",
    "print('The f1_score for this model is: %0.3f +/- %0.3f' % (mean(f1_score), std(accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrain the data using all known data using best model (observed from above values using majority vote): \n",
    "\n",
    "    DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=8,\n",
    "            max_features=None, max_leaf_nodes=None,\n",
    "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "            presort=False, random_state=None, splitter='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=8,\n",
    "            max_features=None, max_leaf_nodes=None,\n",
    "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "            presort=False, random_state=None, splitter='best')\n",
    "data = np.array(knowns.ix[:, knowns.columns != 'label'])\n",
    "dic = {'cruising':1, 'not_cruising':0}\n",
    "target = np.array([dic[n] if n in dic else n for n in np.array(knowns.ix[:, -1])])\n",
    "\n",
    "dt.fit(data, target)\n",
    "joblib.dump(dt, 'models/decision_tree.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = []\n",
    "roc = []\n",
    "f1_score = []\n",
    "recall = []\n",
    "precision = []\n",
    "for i in range(10):\n",
    "    X_train, y_train, X_test, y_test = training_data(knowns, 'False', i)\n",
    "    \n",
    "    # Predicting on the test set\n",
    "    y_pred = dt.predict(X_test)\n",
    "    \n",
    "    # Scoring the performance of the model\n",
    "    accuracy.append(accuracy_score(y_test, y_pred, normalize=True))\n",
    "    roc.append(metrics.roc_auc_score(y_test, y_pred))\n",
    "    f1_score.append(metrics.f1_score(y_test, y_pred))\n",
    "    recall.append(metrics.recall_score(y_test, y_pred))\n",
    "    precision.append(metrics.precision_score(y_test, y_pred))    \n",
    "\n",
    "print('\\n********************** Decision Tree Final Model Performance ********************** ')\n",
    "print('The accuracy for this model is: %0.3f +/- %0.3f' % (mean(accuracy), std(accuracy)))\n",
    "print('The auc_roc for this model is: %0.3f +/- %0.3f' % (mean(roc), std(accuracy)))\n",
    "print('The precision for this model is: %0.3f +/- %0.3f' % (mean(precision), std(accuracy)))\n",
    "print('The recall for this model is: %0.3f +/- %0.3f' % (mean(recall), std(accuracy)))\n",
    "print('The f1_score for this model is: %0.3f +/- %0.3f' % (mean(f1_score), std(accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = pd.DataFrame(metrics.confusion_matrix(y_test, y_pred))\n",
    "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidth=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "plt.title('Decision Tree Confusion Matrix', size = 10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "accuracy = []\n",
    "roc = []\n",
    "f1_score = []\n",
    "recall = []\n",
    "precision = []\n",
    "for i in range(10):\n",
    "    print('\\n********************** For round', i, 'validation set. ********************** ')\n",
    "    X_train, y_train, X_test, y_test = training_data(knowns, 'False', i)\n",
    "    kf = StratifiedKFold(y_train, n_folds=10, shuffle=True, random_state=0)\n",
    "    gridparams = dict(C=list(np.power(10.0, np.arange(-10, 10))))\n",
    "    params = {'penalty':'l2'}\n",
    "    logreg = GridSearchCV(LogisticRegression(**params), gridparams, cv=kf, scoring='f1', n_jobs=-1)\n",
    "    \n",
    "    # Training the model\n",
    "    logreg.fit(X_train, y_train)\n",
    "    print('Best model: ')\n",
    "    print(logreg.best_estimator_)\n",
    "    print(\"\")\n",
    "    \n",
    "    # Predicting on the test set\n",
    "    y_pred = logreg.predict(X_test)\n",
    "    \n",
    "    # Scoring the performance of the model\n",
    "    print(\"Accuracy: %0.3f\" % accuracy_score(y_test, y_pred, normalize=True))\n",
    "    print(\"Aucroc: %0.3f\" % metrics.roc_auc_score(y_test, y_pred))\n",
    "    print(\"f1 score: %0.3f\" % metrics.f1_score(y_test, y_pred))\n",
    "    print(\"Recall: %0.3f\" % metrics.recall_score(y_test, y_pred))\n",
    "    print(\"Precision: %0.3f\" % metrics.precision_score(y_test, y_pred))\n",
    "    accuracy.append(accuracy_score(y_test, y_pred, normalize=True))\n",
    "    roc.append(metrics.roc_auc_score(y_test, y_pred))\n",
    "    f1_score.append(metrics.f1_score(y_test, y_pred))\n",
    "    recall.append(metrics.recall_score(y_test, y_pred))\n",
    "    precision.append(metrics.precision_score(y_test, y_pred))    \n",
    "\n",
    "print('\\n********************** Logistic Regression performance ********************** ')\n",
    "print('The accuracy for this model is: %0.3f +/- %0.3f' % (mean(accuracy), std(accuracy)))\n",
    "print('The auc_roc for this model is: %0.3f +/- %0.3f' % (mean(roc), std(accuracy)))\n",
    "print('The precision for this model is: %0.3f +/- %0.3f' % (mean(precision), std(accuracy)))\n",
    "print('The recall for this model is: %0.3f +/- %0.3f' % (mean(recall), std(accuracy)))\n",
    "print('The f1_score for this model is: %0.3f +/- %0.3f' % (mean(f1_score), std(accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Retrain the data using all known data using best model (observed from above values using majority vote): \n",
    "\n",
    "    LogisticRegression(C=100000000.0, class_weight=None, dual=False,\n",
    "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
    "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
    "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(C=100000000.0, class_weight=None, dual=False,\n",
    "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
    "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
    "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
    "data = np.array(knowns.ix[:, knowns.columns != 'label'])\n",
    "dic = {'cruising':1, 'not_cruising':0}\n",
    "target = np.array([dic[n] if n in dic else n for n in np.array(knowns.ix[:, -1])])\n",
    "\n",
    "logreg.fit(data, target)\n",
    "joblib.dump(logreg, 'models/logistic_regression.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = []\n",
    "roc = []\n",
    "f1_score = []\n",
    "recall = []\n",
    "precision = []\n",
    "for i in range(10):\n",
    "    X_train, y_train, X_test, y_test = training_data(knowns, 'False', i)\n",
    "    \n",
    "    # Predicting on the test set\n",
    "    y_pred = logreg.predict(X_test)\n",
    "    \n",
    "    # Scoring the performance of the model\n",
    "    accuracy.append(accuracy_score(y_test, y_pred, normalize=True))\n",
    "    roc.append(metrics.roc_auc_score(y_test, y_pred))\n",
    "    f1_score.append(metrics.f1_score(y_test, y_pred))\n",
    "    recall.append(metrics.recall_score(y_test, y_pred))\n",
    "    precision.append(metrics.precision_score(y_test, y_pred))    \n",
    "\n",
    "print('\\n********************** Logistic Regression Final Model Performance ********************** ')\n",
    "print('The accuracy for this model is: %0.3f +/- %0.3f' % (mean(accuracy), std(accuracy)))\n",
    "print('The auc_roc for this model is: %0.3f +/- %0.3f' % (mean(roc), std(accuracy)))\n",
    "print('The precision for this model is: %0.3f +/- %0.3f' % (mean(precision), std(accuracy)))\n",
    "print('The recall for this model is: %0.3f +/- %0.3f' % (mean(recall), std(accuracy)))\n",
    "print('The f1_score for this model is: %0.3f +/- %0.3f' % (mean(f1_score), std(accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = pd.DataFrame(metrics.confusion_matrix(y_test, y_pred))\n",
    "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidth=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "plt.title('Logistic Regression Confusion Matrix', size = 10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "accuracy = []\n",
    "roc = []\n",
    "f1_score = []\n",
    "recall = []\n",
    "precision = []\n",
    "for i in range(10):\n",
    "    print('\\n********************** For round', i, 'validation set. ********************** ')\n",
    "    X_train, y_train, X_test, y_test = training_data(knowns, 'False', i)\n",
    "    kf = StratifiedKFold(y_train, n_folds=10, shuffle=True, random_state=0)\n",
    "    gridparams = dict(learning_rate=[0.01, 0.1],loss=['deviance','exponential'])\n",
    "    params = {'n_estimators': 100, 'max_depth': 4}\n",
    "    gbclf = GridSearchCV(ensemble.GradientBoostingClassifier(**params), gridparams, cv=kf, scoring='f1', n_jobs=-1)\n",
    "    \n",
    "    # Training the model\n",
    "    gbclf.fit(X_train, y_train)\n",
    "    print('Best model: ')\n",
    "    print(gbclf.best_estimator_)\n",
    "    print(\"\")\n",
    "    \n",
    "    # Predicting on the test set\n",
    "    y_pred = gbclf.predict(X_test)\n",
    "    \n",
    "    # Scoring the performance of the model\n",
    "    print(\"Accuracy: %0.3f\" % accuracy_score(y_test, y_pred, normalize=True))\n",
    "    print(\"Aucroc: %0.3f\" % metrics.roc_auc_score(y_test, y_pred))\n",
    "    print(\"f1 score: %0.3f\" % metrics.f1_score(y_test, y_pred))\n",
    "    print(\"Recall: %0.3f\" % metrics.recall_score(y_test, y_pred))\n",
    "    print(\"Precision: %0.3f\" % metrics.precision_score(y_test, y_pred))\n",
    "    accuracy.append(accuracy_score(y_test, y_pred, normalize=True))\n",
    "    roc.append(metrics.roc_auc_score(y_test, y_pred))\n",
    "    f1_score.append(metrics.f1_score(y_test, y_pred))\n",
    "    recall.append(metrics.recall_score(y_test, y_pred))\n",
    "    precision.append(metrics.precision_score(y_test, y_pred))    \n",
    "\n",
    "print('\\n********************** Gradient Boosting Classifier performance ********************** ')\n",
    "print('The accuracy for this model is: %0.3f +/- %0.3f' % (mean(accuracy), std(accuracy)))\n",
    "print('The auc_roc for this model is: %0.3f +/- %0.3f' % (mean(roc), std(accuracy)))\n",
    "print('The precision for this model is: %0.3f +/- %0.3f' % (mean(precision), std(accuracy)))\n",
    "print('The recall for this model is: %0.3f +/- %0.3f' % (mean(recall), std(accuracy)))\n",
    "print('The f1_score for this model is: %0.3f +/- %0.3f' % (mean(f1_score), std(accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Retrain the data using all known data using best model (observed from above values using majority vote): \n",
    "\n",
    "    GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
    "              learning_rate=0.1, loss='deviance', max_depth=4,\n",
    "              max_features=None, max_leaf_nodes=None,\n",
    "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "              n_estimators=100, presort='auto', random_state=None,\n",
    "              subsample=1.0, verbose=0, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbclf = ensemble.GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
    "              learning_rate=0.1, loss='deviance', max_depth=4,\n",
    "              max_features=None, max_leaf_nodes=None,\n",
    "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "              n_estimators=100, presort='auto', random_state=None,\n",
    "              subsample=1.0, verbose=0, warm_start=False)\n",
    "data = np.array(knowns.ix[:, knowns.columns != 'label'])\n",
    "dic = {'cruising':1, 'not_cruising':0}\n",
    "target = np.array([dic[n] if n in dic else n for n in np.array(knowns.ix[:, -1])])\n",
    "\n",
    "gbclf.fit(data, target)\n",
    "joblib.dump(gbclf, 'models/gradient_boosting_classifier.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Gradient Boosting Classifier Feature Importance')\n",
    "for f in range(len(knowns.columns[:-1])):\n",
    "    print('\\nFeature: ', knowns.columns[:-1][f], '\\nImportance: ', gbclf.feature_importances_[f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = []\n",
    "roc = []\n",
    "f1_score = []\n",
    "recall = []\n",
    "precision = []\n",
    "for i in range(10):\n",
    "    X_train, y_train, X_test, y_test = training_data(knowns, 'False', i)\n",
    "    \n",
    "    # Predicting on the test set\n",
    "    y_pred = gbclf.predict(X_test)\n",
    "    \n",
    "    # Scoring the performance of the model\n",
    "    accuracy.append(accuracy_score(y_test, y_pred, normalize=True))\n",
    "    roc.append(metrics.roc_auc_score(y_test, y_pred))\n",
    "    f1_score.append(metrics.f1_score(y_test, y_pred))\n",
    "    recall.append(metrics.recall_score(y_test, y_pred))\n",
    "    precision.append(metrics.precision_score(y_test, y_pred))    \n",
    "\n",
    "print('\\n********************** Gradient Boosting Classifier Final Model Performance ********************** ')\n",
    "print('The accuracy for this model is: %0.3f +/- %0.3f' % (mean(accuracy), std(accuracy)))\n",
    "print('The auc_roc for this model is: %0.3f +/- %0.3f' % (mean(roc), std(accuracy)))\n",
    "print('The precision for this model is: %0.3f +/- %0.3f' % (mean(precision), std(accuracy)))\n",
    "print('The recall for this model is: %0.3f +/- %0.3f' % (mean(recall), std(accuracy)))\n",
    "print('The f1_score for this model is: %0.3f +/- %0.3f' % (mean(f1_score), std(accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = pd.DataFrame(metrics.confusion_matrix(y_test, y_pred))\n",
    "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidth=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "plt.title('Gradient Boosting Classifier Confusion Matrix', size = 10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
